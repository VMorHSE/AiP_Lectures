<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
        <meta name="mobile-web-app-capable" content="yes">

		<title>Алгоритмизация и Программирование</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/white.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
	</head>
	<body>
        <div class="reveal">
            <div class="slides">
                <section>
                    <h3>Алгоритмизация и Программирование</h3>
                    <div>&nbsp;</div>
                    <h4>Морозов Владимир Игоревич</h4>
                </section>
                <section>
                    <h4>Асимптотическая сложность алгоритмов</h4>
                </section>
                <section>
                    <section>
                        <h4>Новые единицы оценки алгоритма</h4>
                        <ul>
                            <li>Рассмотренные на прошлой лекции единицы оценки алгоритмов (количество сравнений, обращений к памяти и т.д.) не являются универсальными</li>
                            <li>Вместо этого наиболее часто для оценки алгоритма рассматривается время его работы</li>
                            <li>Реже используется критерий количества памяти, необходимой для работы алгоритма</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Оценка линейного поиска</h4>
                        <ul>
                            <li>Попробуем оценить время работы алгоритма линейного поиска в общем виде</li>
                            <li>Для этого представим время работы каждой строки кода реализации как отдельную константу</li>
                        </ul>
                        <pre><code data-trim class="python">
                            def find(a, x):
                                index = 0 # t_1
                                while a[index] &lt; x: # t_2
                                    index += 1 # t_3
                                return index # t_4
                            </code></pre>
                    </section>
                    <section>
                        <h4>Оценка линейного поиска</h4>
                        <ul>
                            <li>Общее время работы алгоритма в таком случае равно:</li>
                            <p align="center">$t_1 + N * t_2 + (N - 1) * t_3 + t_4$</p>                                            
                            <li>После преобразований получим:</li>
                            <p align="center">$(t_2 + t_3) * N + (t_1 + t_2 - t_3 + t_4)$</p>
                            <li>Заменив множитель перед $N$ на $a_{lin}$, а оставшиеся слагаемые на $b_{lin}$, получим обычную линейную функцию от $N$:</li>
                            <p align="center">$a_{lin} N + b_{lin}$</p>
                        </ul>
                    </section>
                    <section>
                        <h4>Оценка двоичного поиска</h4>
                        <ul>
                            <li>Попробуем оценить время работы алгоритма двоичного поиска в общем виде тем же способом</li>
                        </ul>
                        <pre><code data-trim class="python">
                            def find(a, x):
                                l_1 = 0 # t_1
                                l_2 = len(a) - 1 # t_2
                                while l_1 != l_2: # t_3
                                    k = (l_1 + l_2) // 2 # t_4
                                    if x > a[k]: # t_5
                                        l_1 = k + 1 # t_6
                                    else:
                                        l_2 = k # t_7
                                return l_1 # t_8
                        </code></pre>
                    </section>
                    <section>
                        <h4>Оценка двоичного поиска</h4>
                        <ul>
                            <li>Вспомним, что количество итераций цикла while в худшем случае здесь равно $log_2(N)$ (округление пока опустим)</li>                   
                            <li>Тогда после преобразований получим время работы:</li>
                            <p align="center">$(t_3 + t_4 + t_5) * log_2(N) + t_6 * log_2(N) * c + t_7 * log_2(N) * (1 - c) + (t_1 + t_2 + t_8)$,</p>
                            <p>где $c \in (0, 1)$ – константа, показывающая, в какой доле итераций выполняется условие</p>
                        </ul>
                    </section>
                    <section>
                        <h4>Оценка двоичного поиска</h4>
                        <ul>
                            <li>Преобразовывая далее, получим:</li>
                            <p align="center">$(t_3 + t_4 + t_5 + t_6 * c + t_7 * (1 - c)) * log_2(N) + (t_1 + t_2 + t_3 + t_8)$</p>                
                            <li>Заменив множитель и слагаемое, как в примере с линейным поиском, получим логарифмическую функцию от $N$:</li>
                            <p align="center">$a_{bin} * log_2(N) + b_{bin}$</p>
                        </ul>
                    </section>
                    <section>
                        <h4>Сравнение алгоритмов</h4>
                        <ul>
                            <li>Теперь, когда мы получили время работы алгоритмов в общем виде, для того, чтобы их сравнить, не хватает только конкретных значений констант $a$ и $b$ для каждого из них</li>
                            <li>Покажем, что на самом деле эти значения не важны</li>
                            <li>Пусть $a_{lin} = 1$ и $b_{lin} = 1$</li>
                            <li>Пусть $a_{bin} = 100$ и $b_{bin} = 100$</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Сравнение алгоритмов</h4>
                        <p align="left">
                            Рассмотрим график зависимости времени работы алгоритмов с принятыми значениями констант
                        </p>
                    </section>
                    <section>
                        <p>
                            <img src="02/small_scale_graph.png">
                        </p>
                    </section>
                    <section>
                        <h4>Сравнение алгоритмов</h4>
                        <ul>
                            <li>Очевидно, на малых значениях $N$ линейный поиск гораздо более эффективен</li>
                            <li>Однако при увеличении масштаба картина изменится</li>
                        </ul>
                    </section>
                    <section>
                        <p>
                            <img src="02/big_scale_graph.png">
                        </p>
                    </section>
                    <section>
                        <h4>Сравнение алгоритмов</h4>
                        <ul>
                            <li>Из рассмотренного примера легко увидеть, что, какими бы ни были значения констант, при достаточно большом $N$ линейная функция всегда будет иметь значения большие, чем логарифмичкская</li>
                            <li>В таком случае говорят, что линейная функция <b>доминирует</b> над логарифмической, или что линейная функция имеет более высокий <b>порядок роста</b>, чем логарифмическая</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Сравнение алгоритмов</h4>
                        <ul>
                            <li>Т.к. значения констант, как мы убедились, не важны, при оценке алгоритмов принято говорить об их <b>асимптотической сложности</b>, то есть о сложности при $N \rightarrow \infty$</li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <h4>Верхняя асимптотическая граница</h4>
                        <ul>
                            <li>Зачастую функции сложности алгоритмов имеют необычный вид и с трудом поддаются оценке (пример на следующем слайде)</li>
                            <li>По этой причине для оценки сложности алгоритмов прибегают к оценке её <b>асимптотической верхней границы</b></li>
                            <li>Асимптотически верхняя граница – множество функций, отличающихся только множителем, среди которых всегда можно найти такую, которая всегда будет выше данной на графике правее определённой точки на оси абсцисс</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Верхняя асимптотическая граница</h4>
                        <p>
                            <img src="02/f_and_O.png">
                        </p>
                    </section>
                    <section>
                        <h4>Формальное определение</h4>
                        <ul>
                            <li>Для указания асимптотической верхней границы используются $O$-обозначения</li>
                            <li>$O(g(N))$ означает множество функций таких, что:</li>
                        </ul>
                        <p>
                            $O(g(N)) = \{f(N): \exists c \in {\R}_+, N_0 \in \N \ | \ \forall N \geqslant N_0: 0 \leqslant f(N) \leqslant cg(N)\}$
                        </p>
                    </section>
                    <section>
                        <h4>Определение на русском языке</h4>
                        <p align="left">
                            Таким образом, говорят, что $f(N) \in O(g(N))$ или, что чаще, $f(N) = O(g(N))$, когда можно подобрать такой множитель к функции $g(N)$, что она всегда будет выше функции $f(N)$ на графике правее определённой точки на оси абсцисс
                        </p>
                    </section>
                    <section>
                        <h4>Применение к сложности</h4>
                        <ul>
                            <li>В алгоритмизации наиболее часто встречаются выражения наподобие "сложность алгоритма $O(N)$" или "алгоритм работает за $O(N^2)$"</li>
                            <li>Такие высказывания означают, что функцию сложности алгоритма можно ограничить сверху функцией внутри $O$</li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <h4>Нижняя асимптотическая граница</h4>
                        <ul>
                            <li>Несмотря на то, что верхняя асимптотическая граница худшего случая является наиболее распространённым средством оценки алгоритмов, иногда возникает необходимость найти нижнюю границу для лучшего случая</li>
                            <li>Такая граница используется, как правило, для доказательства того, насколько алгоритм плох, т.к. говорит, что в любой ситуации алгоритм не будет работать лучше, чем указано</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Нижняя асимптотическая граница</h4>
                        <p>
                            <img src="02/f_and_omega.png">
                        </p>
                    </section>
                    <section>
                        <h4>Формальное определение</h4>
                        <ul>
                            <li>Для указания асимптотической нижней границы используются $\Omega$-обозначения</li>
                            <li>$\Omega(g(N))$ означает множество функций таких, что:</li>
                        </ul>
                        <p>
                            $\Omega(g(N)) = \{f(N): \exists c \in {\R}_+, N_0 \in \N \ | \ \forall N \geqslant N_0: 0 \geqslant cg(N) \geqslant f(N)\}$
                        </p>
                    </section>
                    <section>
                        <h4>Определение на русском языке</h4>
                        <p align="left">
                            Таким образом, говорят, что $f(N) \in \Omega(g(N))$ или, что чаще, $f(N) = \Omega(g(N))$, когда можно подобрать такой множитель к функции $g(N)$, что она всегда будет ниже функции $f(N)$ на графике правее определённой точки на оси абсцисс
                        </p>
                    </section>
                </section>
                <section>
                    <section>
                        <h4>Совпадение границ</h4>
                        <ul>
                            <li>Для некоторых алгоритмов верняя и нижняя асимптотические границы совпадают</li>
                            <li>Рассмотрим, например, алгоритм поиска максимального элемента массива</li>
                        </ul>
                        <pre><code data-trim class="python">
                            def find_max(a):
                                res = a[0]
                                for i in range(1, len(a)):
                                    if a[i] > res:
                                        res = a[i]
                                return res
                        </code></pre>
                    </section>
                    <section>
                        <h4>Анализ алгоритма</h4>
                        <ul>
                            <li>Для данного алгоритма в любом случае будет произведено $N - 1$ проверок</li>
                            <li>Такая функция входит в множества $O(N)$ и $\Omega(N)$</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Асимптотически точная оценка</h4>
                        <ul>
                            <li>Если нижняя и верхняя асимптотические оценки функции сложности алгоритма совпадают, мы можем дать <b>асимптотически точную оценку</b> сложности этого алгоритма</li>
                            <li>Такая оценка обозначается $\Theta(N)$</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Формальное определение</h4>
                        <ul>
                            <li>$\Theta(g(N))$ означает множество функций таких, что:</li>
                        </ul>
                        <p>
                            $\Theta(g(N)) = \{f(N): \exists c_1, c_2 \in {\R}_+, N_0 \in \N \ | \ \forall N \geqslant N_0: 0 \leqslant c_1g(N) \leqslant f(N) \leqslant c_2g(N)\}$
                        </p>
                    </section>
                    <section>
                        <h4>Определение на русском языке</h4>
                        <p align="left">
                            Таким образом, говорят, что $f(N) \in \Theta(g(N))$ или, что чаще, $f(N) = \Theta(g(N))$, когда можно подобрать такие множители к функции $g(N)$, что с одним из них она всегда будет ниже функции $f(N)$, а с другим – выше неё на графике правее определённой точки на оси абсцисс
                        </p>
                    </section>
                </section>
                <section>
                    <section>
                        <h4>Распространённые функции сложности</h4>
                        <p align="left">
                            Далее в нашем курсе мы увидим, что большинство алгоритмов имеют функции сложности, входящие в достаточно небольшой список, а именно (по убыванию доминировнаия):
                        </p>
                        <ol>
                            <li>Факториальная – $f(N) = N!$</li>
                            <li>Показательная – $f(N) = c^N, c &gt; 1$</li>
                            <li>Кубическая – $f(N) = N^3$</li>
                            <li>Квадратичная – $f(N) = N^2$</li>
                        </ol>
                    </section>
                    <section>
                        <h4>Распространённые функции сложности</h4>
                        <ol start="5">
                            <li>Суперлинейная – $f(N) = Nlog(N)$</li>
                            <li>Линейная – $f(N) = N$</li>
                            <li>Логарифмическая – $f(N) = log(N)$</li>
                            <li>Константа – $f(N) = 1$</li>
                        </ol>
                    </section>
                    <section>
                        <h4>Таблица скорости роста</h4>
                        <p>
                            <img src="02/growth_table.png">
                        </p>
                    </section>
                </section>
                <section>
                    <h4>Полезные источники</h4>
                    <ol>
                        <li><b>RU</b> Томас Х. Кормен, Чарльз И. Лейзерсон, Рональд Л. Ривест, Клиффорд Штайн. Алгоритмы: построение и анализ, 3-е издание. Главы 3 и 4. – Основная книга нашего курса.</li>
                        <li><b>RU</b> С. Скиена. Алгоритмы. Руководство по разработке. Глава 2. – Более простая книга. Больше примеров, меньше математики.</li>
                        <li><b>RU</b> <a href='https://habr.com/ru/articles/196226/'>Цикл статей на Хабре</a> – Простым языком про сложность. Здесь ссылка на часть 4, но наиболее важны части 1-3 (ссылки внутри)</li>
                    </ol>
                </section>
            </div>
        </div>

		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMath.KaTeX, RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.MathJax2 ]
			});
		</script>
	</body>
</html>